import io
import torch
import librosa
import requests
import tempfile
import numpy as np
import gradio as gr
import soundfile as sf
from transformers import AutoModel
from faster_whisper import WhisperModel

# Function to load reference audio from URL
def load_audio_from_url(url):
    response = requests.get(url)
    if response.status_code == 200:
        audio_data, sample_rate = sf.read(io.BytesIO(response.content))
        return sample_rate, audio_data
    return None, None

# Synthesize speech using IndicF5
def synthesize_speech(text, ref_audio, ref_text):
    if ref_audio is None or ref_text.strip() == "":
        return "Error: Please provide a reference audio and its corresponding text."
    
    # Ensure valid reference audio input
    if isinstance(ref_audio, tuple) and len(ref_audio) == 2:
        sample_rate, audio_data = ref_audio
    else:
        return "Error: Invalid reference audio input."
    
    # Save reference audio directly without resampling
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_audio:
        sf.write(temp_audio.name, audio_data, samplerate=sample_rate, format='WAV')
        temp_audio.flush()
    
    audio = model(text, ref_audio_path=temp_audio.name, ref_text=ref_text)
             
    # Normalize output and save
    if audio.dtype == np.int16:
        audio = audio.astype(np.float32) / 32768.0

    return 24000, audio

# Transcribe reference audio using Faster-Whisper
def transcribe_ref_audio(ref_audio):
    if ref_audio is None:
        return "Error: Please provide a reference audio to transcribe."

    if isinstance(ref_audio, tuple) and len(ref_audio) == 2:
        sample_rate, audio_data = ref_audio
    else:
        return "Error: Invalid reference audio input."

    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_audio:
        sf.write(temp_audio.name, audio_data, samplerate=sample_rate)
        segments, _ = whisper_model.transcribe(temp_audio.name)

    transcription = " ".join([seg.text for seg in segments])
    return transcription

# Load TTS model
repo_id = "ai4bharat/IndicF5"
model = AutoModel.from_pretrained(repo_id, trust_remote_code=True)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# Load Faster-Whisper transcription model
whisper_model = WhisperModel("large-v3", device=device.type, compute_type="int8")

print("Device:", device)

# Example Data (Multiple Examples)
EXAMPLES = [
    {
        "label": "Anjali",
        "audio_url": "https://github.com/Darshit999/indicF5/raw/main/voices/female/anjali.mp3",
        "ref_text": "‡§ï‡•ç‡§Ø‡§æ ‡§π‡§æ‡§≤ ‡§π‡•à ‡§ö‡§æ‡§à ‡§ï‡§æ ‡§ï‡§™ ‡§â‡§†‡§æ‡§ì ‡§î‡§∞ ‡§¨‡•à‡§† ‡§ï‡§∞ ‡§∏‡•Å‡§®‡•ã ‡§¨‡§æ‡§§‡•á‡§Ç ‡§ú‡§¨ ‡§á‡§∏ ‡§Ü‡§µ‡§æ‡§ú ‡§Æ‡•á‡§Ç ‡§π‡•ã‡§Ç‡§ó‡•Ä ‡§§‡•ã ‡§∏‡•Å‡§ï‡•Ç‡§® ‡§∏‡§æ ‡§≤‡§ó‡•á‡§ó‡§æ"
    },
    {
        "label": "Anika",
        "audio_url": "https://github.com/Darshit999/indicF5/raw/main/voices/female/anika.mp3",
        "ref_text": " ‡§õ‡•ã‡§ü‡•Ä ‡§ó‡•Å‡§°‡§ø‡§Ø‡§æ ‡§®‡•á ‡§ú‡•à‡§∏‡•á ‡§π‡•Ä ‡§ú‡§æ‡§¶‡•Ç‡§à ‡§¶‡§∞‡•ç‡§µ‡§æ‡§ú‡§æ ‡§ñ‡•ã‡§≤‡§æ, ‡§è‡§ï ‡§ö‡§Æ‡§ï‡§¶‡§æ‡§∞ ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‡§â‡§∏‡§ï‡•á ‡§∏‡§æ‡§Æ‡§®‡•á ‡§•‡•Ä‡•§ ‡§â‡§∏‡§®‡•á ‡§ï‡§π‡§æ, ‡§ï‡•ç‡§Ø‡§æ ‡§Ø‡§π‡§æ‡§Ç ‡§∏‡§ö ‡§Æ‡•á‡§Ç ‡§¨‡•ã‡§≤‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§ñ‡§∞‡§ó‡•ã‡§∂ ‡§π‡•à‡§Ç‡•§"
    },
    {
        "label": "Diya",
        "audio_url": "https://github.com/Darshit999/indicF5/raw/main/voices/female/diya.mp3",
        "ref_text": " ‡§ú‡•ã ‡§ï‡§π‡§æ‡§®‡•Ä ‡§Æ‡•à‡§Ç ‡§∏‡•Å‡§®‡§æ‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§π‡•Ç‡§Å, ‡§â‡§∏‡§ï‡•Ä ‡§ï‡•ã‡§à ‡§§‡§∏‡•ç‡§µ‡•Ä‡§∞ ‡§®‡§π‡•Ä‡§Ç, ‡§ï‡•ã‡§à ‡§ó‡§µ‡§æ‡§π ‡§®‡§π‡•Ä‡§Ç, ‡§∏‡§ø‡§∞‡•ç‡§´ ‡§è‡§ï ‡§°‡§∞, ‡§ú‡•ã ‡§Ö‡§¨ ‡§§‡§ï ‡§Æ‡•á‡§∞‡•á ‡§Ö‡§Ç‡§¶‡§∞ ‡§ú‡§ø‡§®‡•ç‡§¶‡§æ ‡§π‡•à‡•§"
    },
    {
        "label": "Jessica",
        "audio_url": "https://github.com/Darshit999/indicF5/raw/main/voices/female/jessica.mp3",
        "ref_text": "‡§ú‡•ã ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡§ø ‡§ö‡§ï‡•ç‡§ï‡§∞ ‡§ñ‡§æ ‡§∞‡§π‡§æ ‡§π‡•ã‡§§‡§æ ‡§π‡•à, ‡§µ‡§π ‡§∏‡§Æ‡§ù‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‡§ò‡•Ç‡§Æ ‡§∞‡§π‡•Ä ‡§π‡•à‡•§"
    },
    {
        "label": "Monika",
        "audio_url": "https://github.com/Darshit999/indicF5/raw/main/voices/female/monika.mp3",
        "ref_text": "‡§à‡§≤‡•á‡§µ‡§® ‡§≤‡•á‡§¨‡•ç‡§∏ ‡§ï‡•Ä ‡§∏‡§¨‡§∏‡•á ‡§∏‡§´‡§≤ ‡§≠‡§æ‡§∞‡§§‡§ø‡§Ø ‡§Ü‡§µ‡§æ‡§ú ‡§Ö‡§¨ ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç, ‡§§‡•ã ‡§Ü‡§à‡§è ‡§ï‡•Å‡§õ ‡§¨‡§π‡§§‡§∞‡•Ä‡§® ‡§¨‡§®‡§æ‡§§‡•á ‡§π‡•à‡§Ç‡•§"
    },
    {
        "label": "Muskaan",
        "audio_url": "https://github.com/Darshit999/indicF5/raw/main/voices/female/muskaan.mp3",
        "ref_text": "‡§Ö‡§∞‡•á ‡§¨‡•à‡§†‡•ã ‡§®, ‡§Ü‡§∞‡§æ‡§Æ ‡§∏‡•á ‡§ï‡§π‡§æ‡§®‡•Ä ‡§∏‡•Å‡§®‡§§‡•á ‡§π‡•à‡§Ç, ‡§¨‡§æ‡§§‡•ã‡§Ç ‡§¨‡§æ‡§§‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§∏‡§¨ ‡§∏‡§Æ‡§ù ‡§Ü ‡§ú‡§æ‡§è‡§ó‡§æ"
    },
    {
        "label": "Meher",
        "audio_url": "https://github.com/Darshit999/indicF5/raw/main/voices/female/meher.mp3",
        "ref_text": "‡§µ‡•ã ‡§¶‡§∞‡•ç‡§µ‡§æ‡§ú‡§æ ‡§ß‡•Ä‡§∞‡•á ‡§ß‡•Ä‡§∞‡•á ‡§ñ‡•Å‡§≤‡§æ. ‡§Ö‡§Ç‡§¶‡§∞ ‡§Ö‡§Ç‡§ß‡•á‡§∞‡§æ ‡§•‡§æ. ‡§≤‡•á‡§ï‡§ø‡§® ‡§Ö‡§ú‡•Ä‡§¨ ‡§Ø‡•á ‡§•‡§æ ‡§ï‡§ø ‡§ï‡§ø‡§∏‡•Ä ‡§®‡•á ‡§¶‡§∞‡•ç‡§µ‡§æ‡§ú‡§æ ‡§ñ‡•ã‡§≤‡§æ ‡§®‡§π‡•Ä‡§Ç ‡§•‡§æ."
    },
    
    {
        "label": "Kunal",
        "audio_url": "https://github.com/Darshit999/indicF5/raw/main/voices/male/kunal.mp3",
        "ref_text": "‡§Ö‡§ó‡§∞ ‡§Ü‡§™ ‡§â‡§∏ ‡§∏‡§Æ‡§Ø ‡§Æ‡•Å‡§∏‡•ç‡§ï‡•Å‡§∞‡§æ‡§§‡•á ‡§π‡•à‡§Ç ‡§ú‡§¨ ‡§Ü‡§™‡§ï‡•á ‡§Ü‡§∏‡§™‡§æ‡§∏ ‡§ï‡•ã‡§à ‡§®‡§π‡•Ä‡§Ç ‡§π‡•ã‡§§‡§æ, ‡§§‡•ã ‡§Ü‡§™ ‡§µ‡§æ‡§∏‡•ç‡§§‡§µ ‡§Æ‡•á‡§Ç ‡§á‡§∏‡§ï‡§æ ‡§Æ‡§§‡§≤‡§¨ ‡§∏‡§Æ‡§ù‡§§‡•á ‡§π‡•à‡§Ç‡•§"
    },
    {
        "label": "Yatin",
        "audio_url": "https://github.com/Darshit999/indicF5/raw/main/voices/male/yatin.mp3",
        "ref_text": "‡§è‡§ï ‡§ó‡§π‡§∞‡§æ ‡§∞‡§π‡§∏‡•ç‡§Ø ‡§π‡§µ‡§æ ‡§Æ‡•à ‡§§‡•à‡§∞ ‡§∞‡§π‡§æ ‡§•‡§æ, ‡§π‡§∞ ‡§∏‡§æ‡§Ç‡§∏ ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ï‡§∞‡•Ä‡§¨ ‡§Ü‡§§‡§æ ‡§π‡•Å‡§Ü, ‡§î‡§∞ ‡§ï‡§ø‡§∏‡•Ä ‡§ï‡•ã ‡§®‡§π‡•Ä ‡§™‡§§‡§æ ‡§•‡§æ ‡§ï‡•Ä ‡§Ö‡§ó‡§≤‡•Ä ‡§™‡§≤ ‡§ï‡•ç‡§Ø‡§æ ‡§≤‡•á‡§ï‡§∞ ‡§Ü‡§è‡§ó‡§æ‡•§"
    },
    {
        "label": "Neeraj",
        "audio_url": "https://github.com/Darshit999/indicF5/raw/main/voices/male/neeraj.wav",
        "ref_text": "‡§≠‡§æ‡§∞‡§§‡§ø‡§Ø ‡§ú‡•Ä‡§µ‡§® ‡§ß‡§æ‡§∞‡§æ ‡§Æ‡•á‡§Ç ‡§ú‡§ø‡§® ‡§ó‡•ç‡§∞‡§Ç‡§•‡•ã‡§Ç ‡§ï‡§æ ‡§Æ‡§π‡§§‡•ç‡§µ ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§∏‡•ç‡§•‡§æ‡§® ‡§π‡•à‡•§ ‡§â‡§®‡§Æ‡•á‡§Ç ‡§™‡•Å‡§∞‡§æ‡§® ‡§≠‡§ï‡•ç‡§§‡•Ä ‡§ó‡•ç‡§∞‡§Ç‡§•‡•ã‡§Ç ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§¨‡§π‡•Å‡§§ ‡§Æ‡§π‡§§‡•ç‡§µ ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§Æ‡§æ‡§®‡•á ‡§ú‡§æ‡§§‡•á ‡§π‡•à‡§Ç‡•§"
    },
    {
        "label": "Rudra",
        "audio_url": "https://github.com/Darshit999/indicF5/raw/main/voices/male/rudra.mp3",
        "ref_text": "‡§ï‡§ø‡§§‡§®‡•Ä ‡§Ö‡§ú‡•Ä‡§¨ ‡§¨‡§æ‡§§ ‡§π‡•à ‡§ï‡§ø ‡§Ü‡§ú‡§ï‡§≤ ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§®‡•á ‡§ï‡§ø‡§§‡§æ‡§¨‡•á‡§Ç ‡§™‡§¢‡§º‡§®‡§æ ‡§õ‡•ã‡§°‡§º ‡§π‡•Ä ‡§¶‡§ø‡§Ø‡§æ ‡§π‡•à‡•§ ‡§Ö‡§¨ ‡§ï‡§ø‡§§‡§æ‡§¨‡•á‡§Ç ‡§Ö‡§ó‡§∞ ‡§∏‡•Å‡§®‡§®‡•Ä ‡§π‡•Ä ‡§π‡•à‡§Ç, ‡§§‡•ã ‡§Ö‡§ö‡•ç‡§õ‡•Ä ‡§Ü‡§µ‡§æ‡§ú ‡§Æ‡•á‡§Ç ‡§∏‡•Å‡§®‡•ã‡•§"
    },
    {
        "label": "Viraj",
        "audio_url": "https://github.com/Darshit999/indicF5/raw/main/voices/male/viraj.mp3",
        "ref_text": "‡§ï‡•Å‡§õ ‡§§‡•ã ‡§π‡•à ‡§ú‡•ã ‡§õ‡§ø‡§™‡§æ ‡§π‡•Å‡§Ü ‡§π‡•à‡•§ ‡§π‡§µ‡§æ ‡§Æ‡•á‡§Ç ‡§π‡§≤‡§ï‡•Ä ‡§∏‡•Ä ‡§ó‡§°‡§º-‡§ó‡§°‡§º‡§æ ‡§π‡§ü ‡§ú‡•à‡§∏‡•á ‡§ï‡•ã‡§à ‡§∞‡§æ‡§ú ‡§ñ‡•Å‡§≤‡§®‡•á ‡§µ‡§æ‡§≤‡§æ ‡§π‡•ã‡•§ ‡§ï‡•ç‡§Ø‡§æ ‡§Ü‡§™‡§®‡•á ‡§µ‡•ã ‡§Æ‡§π‡§∏‡•Ç‡§∏ ‡§ï‡§ø‡§Ø‡§æ?"
    },
    {
        "label": "Vikas",
        "audio_url": "https://github.com/Darshit999/indicF5/raw/main/voices/male/vikas.mp3",
        "ref_text": "‡§Ö‡§∞‡•á ‡§π‡§æ, ‡§Æ‡•Å‡§ù‡•á ‡§µ‡•ã ‡§¶‡§ø‡§® ‡§Ö‡§ö‡•ç‡§õ‡•Ä ‡§§‡§∞‡§π ‡§Ø‡§æ‡§¶ ‡§π‡•à, ‡§∏‡§¨ ‡§≤‡•ã‡§ó ‡§á‡§§‡§®‡•á ‡§ñ‡•Å‡§∂ ‡§•‡•á ‡§î‡§∞ ‡§Æ‡§æ‡§π‡•ã‡§≤ ‡§≠‡•Ä ‡§¨‡§π‡•ã‡§§ ‡§ñ‡§æ‡§∏ ‡§•‡§æ‡•§"
    },
    {
        "label": "Arjun",
        "audio_url": "https://github.com/Darshit999/indicF5/raw/main/voices/male/arjun.mp3",
        "ref_text": "‡§Ü‡§™‡§ï‡•ã ‡§™‡§∞‡§ø‡§µ‡§∞‡•ç‡§§‡§® ‡§ï‡•ã ‡§®‡§ø‡§Ø‡§Æ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§∏‡•ç‡§µ‡§æ‡§ó‡§§ ‡§ï‡§∞‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è, ‡§≤‡•á‡§ï‡§ø‡§® ‡§Ö‡§™‡§®‡•á ‡§∂‡§æ‡§∏‡§ï ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§®‡§π‡•Ä‡§Ç."
    },
]


# Preload audio
for ex in EXAMPLES:
    sr, ad = load_audio_from_url(ex["audio_url"])
    ex["sample_rate"] = sr
    ex["audio_data"] = ad

SPEAKER_OPTIONS = {ex["label"]: ex for ex in EXAMPLES}

# On speaker select
def on_speaker_selected(speaker_label):
    ex = SPEAKER_OPTIONS[speaker_label]
    return (ex["sample_rate"], ex["audio_data"]), ex["ref_text"]

# Define Gradio interface with layout adjustments
with gr.Blocks() as iface:
    gr.Markdown(
        """
        # **IndicF5**
        **Supported languages**:  
        **Assamese, Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Odia, Punjabi, Tamil, Telugu.**  
        """
    )
    
    with gr.Row():
        with gr.Column():
            speaker_dropdown = gr.Dropdown(
                choices=list(SPEAKER_OPTIONS.keys()),
                label="Select Reference Speaker"
            )
            text_input = gr.Textbox(label="Text to Synthesize", lines=3)
            ref_audio_input = gr.Audio(type="numpy", label="Reference Prompt Audio")
            ref_text_input = gr.Textbox(label="Text in Reference Prompt Audio", lines=2)
            submit_btn = gr.Button("üé§ Generate Speech", variant="primary")
            transcribe_btn = gr.Button("üìù Transcribe Reference Audio")
        
        with gr.Column():
            output_audio = gr.Audio(label="Generated Speech", type="numpy")
    
    # Event: Load only reference audio & text
    speaker_dropdown.change(
        fn=on_speaker_selected,
        inputs=[speaker_dropdown],
        outputs=[ref_audio_input, ref_text_input]
    )

    submit_btn.click(
        synthesize_speech, 
        inputs=[text_input, ref_audio_input, ref_text_input], 
        outputs=[output_audio]
    )
    
    transcribe_btn.click(
        transcribe_ref_audio,
        inputs=[ref_audio_input],
        outputs=[ref_text_input]
    )

iface.launch(share=True)