import io
import torch
import librosa
import requests
import tempfile
import numpy as np
import gradio as gr
import soundfile as sf
from transformers import AutoModel

# Function to load reference audio from URL
def load_audio_from_url(url):
    response = requests.get(url)
    if response.status_code == 200:
        audio_data, sample_rate = sf.read(io.BytesIO(response.content))
        return sample_rate, audio_data
    return None, None

def synthesize_speech(text, ref_audio, ref_text):
    if ref_audio is None or ref_text.strip() == "":
        return "Error: Please provide a reference audio and its corresponding text."
    
    # Ensure valid reference audio input
    if isinstance(ref_audio, tuple) and len(ref_audio) == 2:
        sample_rate, audio_data = ref_audio
    else:
        return "Error: Invalid reference audio input."
    
    # Save reference audio directly without resampling
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_audio:
        sf.write(temp_audio.name, audio_data, samplerate=sample_rate, format='WAV')
        temp_audio.flush()
    
    audio = model(text, ref_audio_path=temp_audio.name, ref_text=ref_text)
             
    # Normalize output and save
    if audio.dtype == np.int16:
        audio = audio.astype(np.float32) / 32768.0

    return 24000, audio


# Load TTS model
repo_id = "ai4bharat/IndicF5"
model = AutoModel.from_pretrained(repo_id, trust_remote_code=True)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device", device)
model = model.to(device)

# Example Data (Multiple Examples)
EXAMPLES = [
    {
        "label": "Anjali",
        "audio_url": "https://github.com/Darshit999/indicF5/raw/main/voices/female/anjali.wav",
        "ref_text": "‡§ï‡•ç‡§Ø‡§æ ‡§π‡§æ‡§≤ ‡§π‡•à ‡§ö‡§æ‡§à ‡§ï‡§æ ‡§ï‡§™ ‡§â‡§†‡§æ‡§ì ‡§î‡§∞ ‡§¨‡•à‡§† ‡§ï‡§∞ ‡§∏‡•Å‡§®‡•ã ‡§¨‡§æ‡§§‡•á‡§Ç ‡§ú‡§¨ ‡§á‡§∏ ‡§Ü‡§µ‡§æ‡§ú ‡§Æ‡•á‡§Ç ‡§π‡•ã‡§Ç‡§ó‡•Ä ‡§§‡•ã ‡§∏‡•Å‡§ï‡•Ç‡§® ‡§∏‡§æ ‡§≤‡§ó‡•á‡§ó‡§æ"
    },
    {
        "label": "Jessica",
        "audio_url": "https://github.com/Darshit999/indicF5/raw/main/voices/female/jessica.mp3",
        "ref_text": "‡§ú‡•ã ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡§ø ‡§ö‡§ï‡•ç‡§ï‡§∞ ‡§ñ‡§æ ‡§∞‡§π‡§æ ‡§π‡•ã‡§§‡§æ ‡§π‡•à, ‡§µ‡§π ‡§∏‡§Æ‡§ù‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‡§ò‡•Ç‡§Æ ‡§∞‡§π‡•Ä ‡§π‡•à‡•§"
    },
    {
        "label": "Monika",
        "audio_url": "https://github.com/Darshit999/indicF5/raw/main/voices/female/monika.mp3",
        "ref_text": "‡§à‡§≤‡•á‡§µ‡§® ‡§≤‡•á‡§¨‡•ç‡§∏ ‡§ï‡•Ä ‡§∏‡§¨‡§∏‡•á ‡§∏‡§´‡§≤ ‡§≠‡§æ‡§∞‡§§‡§ø‡§Ø ‡§Ü‡§µ‡§æ‡§ú ‡§Ö‡§¨ ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç, ‡§§‡•ã ‡§Ü‡§à‡§è ‡§ï‡•Å‡§õ ‡§¨‡§π‡§§‡§∞‡•Ä‡§® ‡§¨‡§®‡§æ‡§§‡•á ‡§π‡•à‡§Ç‡•§"
    },
    {
        "label": "Muskaan",
        "audio_url": "https://github.com/Darshit999/indicF5/raw/main/voices/female/muskaan.mp3",
        "ref_text": "‡§Ö‡§∞‡•á ‡§¨‡•à‡§†‡•ã ‡§®, ‡§Ü‡§∞‡§æ‡§Æ ‡§∏‡•á ‡§ï‡§π‡§æ‡§®‡•Ä ‡§∏‡•Å‡§®‡§§‡•á ‡§π‡•à‡§Ç, ‡§¨‡§æ‡§§‡•ã‡§Ç ‡§¨‡§æ‡§§‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§∏‡§¨ ‡§∏‡§Æ‡§ù ‡§Ü ‡§ú‡§æ‡§è‡§ó‡§æ"
    },
    {
        "label": "Kunal",
        "audio_url": "https://github.com/Darshit999/indicF5/raw/main/voices/male/kunal.mp3",
        "ref_text": " ‡§Ö‡§ó‡§∞ ‡§Ü‡§™ ‡§â‡§∏ ‡§∏‡§Æ‡§Ø ‡§Æ‡•Å‡§∏‡•ç‡§ï‡•Å‡§∞‡§æ‡§§‡•á ‡§π‡•à‡§Ç ‡§ú‡§¨ ‡§Ü‡§™‡§ï‡•á ‡§Ü‡§∏‡§™‡§æ‡§∏ ‡§ï‡•ã‡§à ‡§®‡§π‡•Ä‡§Ç ‡§π‡•ã‡§§‡§æ, ‡§§‡•ã ‡§Ü‡§™ ‡§µ‡§æ‡§∏‡•ç‡§§‡§µ ‡§Æ‡•á‡§Ç ‡§á‡§∏‡§ï‡§æ ‡§Æ‡§§‡§≤‡§¨ ‡§∏‡§Æ‡§ù‡§§‡•á ‡§π‡•à‡§Ç‡•§"
    },
    {
        "label": "Neeraj",
        "audio_url": "https://github.com/Darshit999/indicF5/raw/main/voices/male/neeraj.wav",
        "ref_text": " ‡§≠‡§æ‡§∞‡§§‡§ø‡§Ø ‡§ú‡•Ä‡§µ‡§® ‡§ß‡§æ‡§∞‡§æ ‡§Æ‡•á‡§Ç ‡§ú‡§ø‡§® ‡§ó‡•ç‡§∞‡§Ç‡§•‡•ã‡§Ç ‡§ï‡§æ ‡§Æ‡§π‡§§‡•ç‡§µ ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§∏‡•ç‡§•‡§æ‡§® ‡§π‡•à‡•§ ‡§â‡§®‡§Æ‡•á‡§Ç ‡§™‡•Å‡§∞‡§æ‡§® ‡§≠‡§ï‡•ç‡§§‡•Ä ‡§ó‡•ç‡§∞‡§Ç‡§•‡•ã‡§Ç ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§¨‡§π‡•Å‡§§ ‡§Æ‡§π‡§§‡•ç‡§µ ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§Æ‡§æ‡§®‡•á ‡§ú‡§æ‡§§‡•á ‡§π‡•à‡§Ç‡•§"
    },
    {
        "label": "Rudra",
        "audio_url": "https://github.com/Darshit999/indicF5/raw/main/voices/male/rudra.mp3",
        "ref_text": " ‡§ï‡§ø‡§§‡§®‡•Ä ‡§Ö‡§ú‡•Ä‡§¨ ‡§¨‡§æ‡§§ ‡§π‡•à ‡§ï‡§ø ‡§Ü‡§ú‡§ï‡§≤ ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§®‡•á ‡§ï‡§ø‡§§‡§æ‡§¨‡•á‡§Ç ‡§™‡§¢‡§º‡§®‡§æ ‡§õ‡•ã‡§°‡§º ‡§π‡•Ä ‡§¶‡§ø‡§Ø‡§æ ‡§π‡•à‡•§ ‡§Ö‡§¨ ‡§ï‡§ø‡§§‡§æ‡§¨‡•á‡§Ç ‡§Ö‡§ó‡§∞ ‡§∏‡•Å‡§®‡§®‡•Ä ‡§π‡•Ä ‡§π‡•à‡§Ç, ‡§§‡•ã ‡§Ö‡§ö‡•ç‡§õ‡•Ä ‡§Ü‡§µ‡§æ‡§ú ‡§Æ‡•á‡§Ç ‡§∏‡•Å‡§®‡•ã‡•§"
    },
    {
        "label": "Viraj",
        "audio_url": "https://github.com/Darshit999/indicF5/raw/main/voices/male/viraj.mp3",
        "ref_text": "‡§ï‡•Å‡§õ ‡§§‡•ã ‡§π‡•à ‡§ú‡•ã ‡§õ‡§ø‡§™‡§æ ‡§π‡•Å‡§Ü ‡§π‡•à‡•§ ‡§π‡§µ‡§æ ‡§Æ‡•á‡§Ç ‡§π‡§≤‡§ï‡•Ä ‡§∏‡•Ä ‡§ó‡§°‡§º-‡§ó‡§°‡§º‡§æ ‡§π‡§ü ‡§ú‡•à‡§∏‡•á ‡§ï‡•ã‡§à ‡§∞‡§æ‡§ú ‡§ñ‡•Å‡§≤‡§®‡•á ‡§µ‡§æ‡§≤‡§æ ‡§π‡•ã‡•§ ‡§ï‡•ç‡§Ø‡§æ ‡§Ü‡§™‡§®‡•á ‡§µ‡•ã ‡§Æ‡§π‡§∏‡•Ç‡§∏ ‡§ï‡§ø‡§Ø‡§æ?"
    },
]


# Preload audio
for ex in EXAMPLES:
    sr, ad = load_audio_from_url(ex["audio_url"])
    ex["sample_rate"] = sr
    ex["audio_data"] = ad

SPEAKER_OPTIONS = {ex["label"]: ex for ex in EXAMPLES}

# On speaker select
def on_speaker_selected(speaker_label):
    ex = SPEAKER_OPTIONS[speaker_label]
    return (ex["sample_rate"], ex["audio_data"]), ex["ref_text"]

# Define Gradio interface with layout adjustments
with gr.Blocks() as iface:
    gr.Markdown(
        """
        # **IndicF5**
        **Supported languages**:  
        **Assamese, Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Odia, Punjabi, Tamil, Telugu.**  
        """
    )
    
    with gr.Row():
        with gr.Column():
            speaker_dropdown = gr.Dropdown(
                choices=list(SPEAKER_OPTIONS.keys()),
                label="Select Reference Speaker"
            )
            text_input = gr.Textbox(label="Text to Synthesize", lines=3)
            ref_audio_input = gr.Audio(type="numpy", label="Reference Prompt Audio")
            ref_text_input = gr.Textbox(label="Text in Reference Prompt Audio", lines=2)
            submit_btn = gr.Button("üé§ Generate Speech", variant="primary")
        
        with gr.Column():
            output_audio = gr.Audio(label="Generated Speech", type="numpy")
    
    # Event: Load only reference audio & text
    speaker_dropdown.change(
        fn=on_speaker_selected,
        inputs=[speaker_dropdown],
        outputs=[ref_audio_input, ref_text_input]
    )

    submit_btn.click(
        synthesize_speech, 
        inputs=[text_input, ref_audio_input, ref_text_input], 
        outputs=[output_audio]
    )


iface.launch(share=True)